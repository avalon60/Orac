[service]
# Define which LLM service we are running against (e.g. lmstudio, ollama, openai)
llm_service_id = ollama
# Default model - loaded at startup
# default_model_name = hermes-3-llama-3.2-3b
default_model_name = deepseek-r1:14b
# LM Studio - port 1234
# service_url = http://localhost:1234
# Ollama - port 11434
service_url = http://localhost:11434 

[settings]
# Unless overridden by show_reasoning in the API JSON payload, we define whether or 
# not to strip reasoning narrative here.
strip_reasoning_tags = false

[vector_db]
# Store chunks of 3 turns
granularity = chunk_3
# Ignore very short messages
min_tokens = 10 
# Ignore “thanks”, “ok”, etc.
skip_trivial = yes 

# Summarize older history
compress_old_sessions = yes
max_size_gb = 250 # Total DB space
