[global]
project_identifier = Orac
[service]
# Define which LLM service we are running against (e.g. lmstudio, ollama, openai)
llm_service_id = ollama
# Default model (LM Studio) - loaded at startup
# default_model_name = meta-llama-3.1-8b-instruct
# Default model (Ollama) - loaded at startup
default_model_name = deepseek-r1:14b
# LM Studio - port 1234
# service_url = http://localhost:1234
# Ollama - port 11434
service_url = http://localhost:11434 

[settings]
# Unless overridden by show_reasoning in the API JSON payload, we define whether or 
# not to strip reasoning narrative here.
strip_reasoning_tags = false

[vector_db]
# Store chunks of 3 turns
granularity = chunk_3
# Ignore very short messages
min_tokens = 10 
# Ignore “thanks”, “ok”, etc.
skip_trivial = yes 

# Summarize older history
compress_old_sessions = yes
max_size_gb = 250 # Total DB space

[logging]
# Enable or disable log stamping (adds a timestamp-based ID to logs)
log_stamping = true

# Whether to include stderr output (true/false)
inc_stderr = false

# Default log level (TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL)
log_level = INFO

# Optional custom colours for log levels (if you use them)
supplementary = light-green
feature = cyan


